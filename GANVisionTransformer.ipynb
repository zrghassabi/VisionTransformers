{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPkdhWnY0jUksi/t0wJoK9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/VisionTransformers/blob/main/GANVisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining Generative Adversarial Networks (GANs) with Vision Transformers (ViTs) creates a powerful framework for various image generation tasks. GANs are used to generate realistic images, while ViTs can help enhance the quality of generated images or provide better representations for tasks like super-resolution or inpainting.\n",
        "\n",
        "Hereâ€™s a high-level approach to combining GANs with Vision Transformers:\n",
        "\n",
        "#Define the Vision Transformer Architecture:\n",
        "\n",
        "This could be used in either the generator or discriminator, or both.\n",
        "\n",
        "#Create the GAN Framework:\n",
        "Set up the GAN components: the generator, discriminator, and the adversarial training process.\n",
        "\n",
        "#Integrate the Vision Transformer: Incorporate the Vision Transformer into the GAN architecture.\n",
        "\n",
        "#Example Code\n",
        "\n",
        "Below is an example that integrates Vision Transformers into a GAN framework. In this example, the Vision Transformer is used in the discriminator to enhance its capability in image classification.\n",
        "\n",
        "Vision Transformer Discriminator for GAN\n",
        "\n",
        "# 1 Vision Transformer Block"
      ],
      "metadata": {
        "id": "Vmez9CrHMxmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, num_layers):\n",
        "        super(VisionTransformerBlock, self).__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)  # [num_patches, batch_size, hidden_dim]\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x.permute(1, 0, 2)  # [batch_size, num_patches, hidden_dim]\n"
      ],
      "metadata": {
        "id": "NjDxBZALNYH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Discriminator with Vision Transformer"
      ],
      "metadata": {
        "id": "il9OhY_1NoVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-hBLQoKAMaUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTDiscriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=8, in_channels=3, hidden_dim=256, num_heads=4, num_layers=4):\n",
        "        super(ViTDiscriminator, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * in_channels\n",
        "\n",
        "        # Patch Embedding Layer\n",
        "        self.patch_embedding = nn.Linear(self.patch_dim, hidden_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
        "\n",
        "        # Vision Transformer Block\n",
        "        self.vit_block = VisionTransformerBlock(hidden_dim, num_heads, num_layers)\n",
        "\n",
        "        # Classification Head\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Create patches\n",
        "        batch_size = x.size(0)\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.contiguous().view(batch_size, self.patch_size * self.patch_size * x.size(1), -1)\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, num_patches, patch_dim]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # Ensure positional encoding is properly sized\n",
        "        if self.position_embedding.size(1) != x.size(1):\n",
        "            self.position_embedding = nn.Parameter(torch.zeros(1, x.size(1), x.size(2)))\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.position_embedding\n",
        "\n",
        "        # Vision Transformer Block\n",
        "        x = self.vit_block(x)\n",
        "\n",
        "        # Use the output of the class token\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jsRgxYQWNu4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- GAN Framework"
      ],
      "metadata": {
        "id": "z9_N8t9tOOBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, img_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, 128 * 8 * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self, z_dim=100, img_size=64):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = Generator(z_dim=z_dim)\n",
        "        self.discriminator = ViTDiscriminator(img_size=img_size)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def discriminate(self, img):\n",
        "        return self.discriminator(img)\n"
      ],
      "metadata": {
        "id": "CjYaydQOOT5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Training the GAN"
      ],
      "metadata": {
        "id": "coaeswjzOm7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DataLoader Setup\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Initialize the GAN\n",
        "z_dim = 100\n",
        "gan = GAN(z_dim=z_dim, img_size=64)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_g = optim.Adam(gan.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(gan.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    for real_images, _ in train_loader:\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        outputs = gan.discriminate(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        d_loss_real.backward()\n",
        "\n",
        "        z = torch.randn(batch_size, z_dim)\n",
        "        fake_images = gan(z)\n",
        "        outputs = gan.discriminate(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        d_loss_fake.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        outputs = gan.discriminate(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, D Loss: {d_loss_real.item() + d_loss_fake.item()}, G Loss: {g_loss.item()}')\n"
      ],
      "metadata": {
        "id": "51pqOT4tOs1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points:\n",
        "\n",
        "#Vision Transformer Block:\n",
        "Defines a transformer block used in the discriminator for learning image features.\n",
        "#ViT Discriminator:\n",
        "Uses Vision Transformer to classify real vs. fake images.\n",
        "\n",
        "#Generator:\n",
        "Simple fully connected network to generate images from random noise.\n",
        "\n",
        "#Training:\n",
        "Train the discriminator to differentiate between real and generated images, and train the generator to produce realistic images that can fool the discriminator.\n",
        "\n",
        "This setup leverages the Vision Transformer for feature extraction and classification within a GAN framework, improving image quality and representation learning. You can adapt the architecture and training procedure to suit specific use cases or datasets."
      ],
      "metadata": {
        "id": "VBlZtCqVPKee"
      }
    }
  ]
}