{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO7c9mlNg1xQPXBGGzvAbN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/VisionTransformers/blob/main/VisionTransformersBasics1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's a basic illustration of the Vision Transformer (ViT) architecture with corresponding code snippets in PyTorch. The Vision Transformer processes images by dividing them into patches, applying attention mechanisms, and then using feed-forward networks for feature extraction.\n",
        "\n",
        "Overview of Vision Transformer Structure\n",
        "#Patch Embedding:\n",
        " Divide the image into fixed-size patches and embed each patch into a vector.\n",
        "#Positional Encoding:\n",
        "Add positional information to the patch embeddings.\n",
        "#Transformer Encoder Layers:\n",
        "Apply multiple layers of the transformer encoder, including multi-head self-attention and feed-forward networks.\n",
        "#Classification Head:\n",
        " Apply a final layer to the output of the transformer encoder to produce the classification output."
      ],
      "metadata": {
        "id": "JHluvmI6EWf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6b4VkaRESSX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, hidden_dim=768, num_heads=12, num_layers=12):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * in_channels\n",
        "\n",
        "        # Patch Embedding Layer\n",
        "        self.patch_embedding = nn.Linear(self.patch_dim, hidden_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, hidden_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification Head\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, channels, height, width]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Create patches\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.contiguous().view(batch_size, self.patch_size * self.patch_size * x.size(1), -1)\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, num_patches, patch_dim]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches + 1, hidden_dim]\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.position_embedding\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = x.permute(1, 0, 2)  # [num_patches + 1, batch_size, hidden_dim]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[0]  # Take the output of the class token\n",
        "\n",
        "        # Classification head\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "model = VisionTransformer(img_size=224, patch_size=16, num_classes=1000)\n",
        "input_tensor = torch.randn(8, 3, 224, 224)  # Example batch of images\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Output shape should be [8, 1000] for 8 images and 1000 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "#Patch Embedding:\n",
        "The image is divided into patches of size patch_size x patch_size, and each patch is flattened and linearly embedded into a vector of size hidden_dim.\n",
        "\n",
        "#Positional Encoding:\n",
        " Positional information is added to the patch embeddings to retain spatial information.\n",
        "\n",
        "#Transformer Encoder:\n",
        " Multiple layers of the transformer encoder are applied, which include multi-head self-attention and feed-forward layers.\n",
        "\n",
        "#Classification Head:\n",
        "The output corresponding to the class token (the first token) is passed through a final linear layer to produce the class scores."
      ],
      "metadata": {
        "id": "bvgx24osFgE7"
      }
    }
  ]
}