{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM5b3DD/VSKj3LMaVFBEpzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/VisionTransformers/blob/main/VisionTransformersBasics1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's a basic illustration of the Vision Transformer (ViT) architecture with corresponding code snippets in PyTorch. The Vision Transformer processes images by dividing them into patches, applying attention mechanisms, and then using feed-forward networks for feature extraction.\n",
        "\n",
        "Overview of Vision Transformer Structure\n",
        "#Patch Embedding:\n",
        " Divide the image into fixed-size patches and embed each patch into a vector.\n",
        "#Positional Encoding:\n",
        "Add positional information to the patch embeddings.\n",
        "#Transformer Encoder Layers:\n",
        "Apply multiple layers of the transformer encoder, including multi-head self-attention and feed-forward networks.\n",
        "#Classification Head:\n",
        " Apply a final layer to the output of the transformer encoder to produce the classification output."
      ],
      "metadata": {
        "id": "JHluvmI6EWf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6b4VkaRESSX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, hidden_dim=768, num_heads=12, num_layers=12):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * in_channels\n",
        "\n",
        "        # Patch Embedding Layer\n",
        "        self.patch_embedding = nn.Linear(self.patch_dim, hidden_dim)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, hidden_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification Head\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, channels, height, width]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Create patches\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.contiguous().view(batch_size, self.patch_size * self.patch_size * x.size(1), -1)\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, num_patches, patch_dim]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches + 1, hidden_dim]\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.position_embedding\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = x.permute(1, 0, 2)  # [num_patches + 1, batch_size, hidden_dim]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[0]  # Take the output of the class token\n",
        "\n",
        "        # Classification head\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "model = VisionTransformer(img_size=224, patch_size=16, num_classes=1000)\n",
        "input_tensor = torch.randn(8, 3, 224, 224)  # Example batch of images\n",
        "output = model(input_tensor)\n",
        "print(output.shape)  # Output shape should be [8, 1000] for 8 images and 1000 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "#Patch Embedding:\n",
        "The image is divided into patches of size patch_size x patch_size, and each patch is flattened and linearly embedded into a vector of size hidden_dim.\n",
        "\n",
        "#Positional Encoding:\n",
        " Positional information is added to the patch embeddings to retain spatial information.\n",
        "\n",
        "#Transformer Encoder:\n",
        " Multiple layers of the transformer encoder are applied, which include multi-head self-attention and feed-forward layers.\n",
        "\n",
        "#Classification Head:\n",
        "The output corresponding to the class token (the first token) is passed through a final linear layer to produce the class scores."
      ],
      "metadata": {
        "id": "bvgx24osFgE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! To use the Vision Transformer (ViT) model for image classification, you'll need to follow these steps:\n",
        "\n",
        "#Prepare the Dataset:\n",
        "Load and preprocess your image data.\n",
        "#Define the Model:\n",
        "Use the Vision Transformer architecture.\n",
        "#Train the Model:\n",
        "Implement the training loop.\n",
        "#Evaluate the Model:\n",
        " Test the model on validation or test data.\n",
        "\n",
        "Here's a complete example using the Vision Transformer model for image classification in PyTorch. This example assumes you have a dataset and are familiar with basic PyTorch operations."
      ],
      "metadata": {
        "id": "pw_pNNxVGdgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set up data transformations and dataloaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = VisionTransformer(img_size=224, patch_size=16, num_classes=10)  # CIFAR-10 has 10 classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "m6dEX_dbL2fF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}